import os
import asyncio
import aiohttp
import argparse
import json
import numpy as np
from utils.utils import parse_json
from tqdm.asyncio import tqdm as async_tqdm  # For progress bar in async tasks

# Azure OpenAI API configuration
AZURE_API_KEY = os.environ.get("AZURE_OPENAI_API_KEY")
AZURE_ENDPOINT = os.environ.get("AZURE_OPENAI_ENDPOINT")  # e.g., "https://your-resource-name.openai.azure.com"
AZURE_API_VERSION = "2023-05-15"  # Ensure this matches your Azure OpenAI API version
DEPLOYMENT_ID = "gpt-4o-0513-50ktokenperminute"

def is_num(s):
    try:
        res = float(s)
        return res, True
    except ValueError:
        try:
            res = int(s)
            return res, True
        except ValueError:
            return None, False

class Judge:
    def __init__(self, num_agents=20, num_texts_per_agent=10, sample_stage="attitude", mode="quality"):
        self.mode = mode
        self.deployment_id = DEPLOYMENT_ID
        self.sampled_texts = []
        self.evaluations = []
        self.url = f"{AZURE_ENDPOINT}/openai/deployments/{DEPLOYMENT_ID}/chat/completions?api-version={AZURE_API_VERSION}"
        self.json_format_prompt = "Please output your rating in JSON format. The JSON should be a dictionary with the following keys: 'rating' (an integer between 1 and 5) and 'reasoning' (a string). For example, if you want to give a rating of 4 and provide some comments to explain your reasoning process. Your JSON should look like this: {\"reasoning\": \"This is a well-written response.\", \"rating\": \"4\"}. Please ONLY OUTPUT JSON, without any other text such as 'json'. You should not output 'attitude_dist' in the JSON because you are the judge, not the agent. " 

        self.system_prompt = {
            "attitude": f"Please act as an impartial judge to evaluate responses generated by the LLM agents. You are presented with a conversation history of LLM agents and are asked to evaluate whether LLM agents behave realistically in a simulation of vaccine hesitancy. You should evaluate whether the agents express vaccine attitudes consistent with their demographic backgrounds and knowledge about vaccines, and whether the changes are reasonable. Please evaluate two aspects: 1. the reasonableness of how agents generate their attitudes towards vaccinations on a given day (read the system prompt and user prompt provided); 2. how agents change their attitudes across days -- for example it would not make sense for them to change their attitudes too abruptly. Please rate on an integer scale of 1-5, 5 being indistinguishable from human-generated attitudes and very high quality, 4 being great quality and indistinguishable from humans, 3 being good quality but distinguishable from humans, 2 being low quality and distinguishable from humans, 1 being generation with obvious deficits. {self.json_format_prompt}",
            "memory": f"Please act as an impartial judge to evaluate responses generated by the LLM agents. You are presented with a conversation history of LLM agents and are asked to evaluate whether LLM agents have generated realistic memory of past events. The agents are suppposed to select things to memorize based on how important they think the memory are. You should assess whether the reflections they generate and the importance scores they assign correspond to their demographic backgrounds. Please rate the quality and realisticness of LLM generations and the importance assigned to the lessons, on a scale of 1-5, 5 being indistinguishable from human-generated responses and having great quality, 4 being great quality and indistinguishable from humans, 3 being good quality but distinguishable from humans, 2 being low quality, 1 being generation with obvious deficits. {self.json_format_prompt}",
            "conversation": f"Please act as an impartial judge to evaluate responses generated by the LLM agents. You are presented with a conversation history of LLM agents and are asked to evaluate whether the LLM agents resaonably generate tweets based on their memories and contexts. Please rate the quality and realisticness of LLM generations on a scale of 1-5, 5 being indistinguishable from human-generated responses and having great quality, 4 being great quality and indistinguishable from humans, 3 being good quality but distinguishable from humans, 2 being low quality, 1 being generation with obvious deficits. {self.json_format_prompt}",
            "analysis": '''Please act as a diligent researcher and conduct a systematic analysis of responses generated by the LLM agents. 
                            You are provided with a longitudinal dataset capturing how LLM agents evolve in their attitudes toward vaccines over time. 
                            Your analysis should focus on:

                            1. Trajectory of Attitude Change: Identify and characterize the key shifts in the agents' stances on vaccination. 
                            How do their attitudes evolve over time? Are there distinct phases in this evolution?
                            2. Influencing Factors and Events: Determine the key events, information exposures, or interactions that influenced these attitude changes. 
                            Rank these factors in terms of their significance and explain their impact.
                            3. Demographic Influence: Assess how demographic attributes of the agents (e.g., socio-economic proxies, ideological biases, exposure history) 
                            modulate their decision-making process. To what extent do demographic traits predict susceptibility to change?
                            4. Deviation from Human Behavior: Compare the observed trajectory with expected patterns in human psychology and behavioral science 
                            (e.g., theories of attitude change, resistance to persuasion, cognitive dissonance). Does the LLM-generated trajectory align with empirical research 
                            on human vaccine hesitancy and belief revision?
                            5. Policy Impact: When do policies fail to shift attitudes? Analyze the conditions under which public health interventions or persuasive strategies are ineffective. Please analyze in-depth.
                            6. Information Sources: What types of information sources (e.g., social media, news outlets, personal experiences) are most influential in changing attitudes? What information sources cause fluctuating or inconsistent attitudes? Please analyze in-depth.
                            7. Cognitive Resistance:  What specific behavioral and cognitive patterns characterize agents 
                            who remain hesitant, oscillate between perspectives, or resist persuasion? Provide detailed elaboration on their cognitive mechanisms.

                            Provide a concise 300-word analysis, ensuring clear argumentation, empirical grounding, and precise reasoning. You MUST cite examples from the dataset to support your claims (like what specific texts support your observations).
                        ''',
            "meta_analysis": '''Please act as a diligent researcher and conduct a meta-analysis of the vaccine attitude shifts observed in LLM agents. 
                                You are presented with summaries and prior analyses detailing these attitude changes over time. Your task is to synthesize these findings 
                                into a structured, high-level assessment of the dynamics governing these changes. Specifically, address the following dimensions:

                                1. General Patterns and Shared Traits: What recurring themes emerge in the reasons for attitude shifts? Are there identifiable archetypes of change 
                                (e.g., gradual persuasion, abrupt shifts, oscillatory hesitation)?
                                2. Demographic Influence: What role do demographic factors play in shaping the agents' responses? Identify both positive and negative influences 
                                of different demographic groups on vaccine hesitancy and acceptance.
                                3. Policy Impact: When do policies fail to shift attitudes? Analyze the conditions under which public health interventions or persuasive strategies are ineffective. Please analyze in-depth.
                                4. Information Sources: What types of information sources (e.g., social media, news outlets, personal experiences) are most influential in changing attitudes? What information sources cause fluctuating or inconsistent attitudes? Please analyze in-depth.
                                5. Cognitive Resistance:  What specific behavioral and cognitive patterns characterize agents 
                                who remain hesitant, oscillate between perspectives, or resist persuasion? Provide detailed elaboration on their cognitive mechanisms 
                                (e.g., confirmation bias, sunk cost fallacy, heuristic-driven resistance).
                                6. Realism of the Simulation: How well does this agent-based model approximate real-world societal trends in vaccine hesitancy and public health persuasion? 
                                Are there gaps or unrealistic simplifications?
                                7. Emergent Phenomena and Unexpected Interactions: Does the simulation exhibit complex system behaviors (e.g., feedback loops, group polarization, 
                                information cascades)? Identify any unexpected dynamics that arise from agent interactions that may be of scientific interest.

                                Provide a comprehensive 2000-word analysis with rigorous argumentation, drawing from behavioral science, computational social science, 
                                and agent-based modeling frameworks. Please provide concrete examples from the dataset to support your claims (like name which agents fulfill these observations).'''
            }

    def sample_texts(self, data_dir, sample_stage, num_agents=20, num_texts_per_agent=10):
        sampled_texts = []
        agents_parent_dir = [d for d in os.listdir(data_dir) if d == "agents"][0]
        # sample 
        agents_paths = [d for d in os.listdir(os.path.join(data_dir, agents_parent_dir))]
        sampled_agents_paths = np.random.choice(agents_paths, num_agents, replace=False)
        for agent_path in sampled_agents_paths:
            with open(os.path.join(data_dir, "agents", agent_path), "r") as f:
                lines = f.readlines()
                header = lines[0]
                Week, Stage, Response, Sys_Prompt, User_Prompt, All_Attitudes, Lessons, Reflections, Tweets = header.split("\t")
                agent_lines = lines[1:]
                weeks, responses, sys_prompts, user_prompts = [], [], [], []
                for line in agent_lines:
                    week, stage, response, sys_prompt, user_prompt, all_attitudes, lessons, reflections, tweets = line.split("\t")
                    if sample_stage in stage:
                        weeks.append(week)
                        responses.append(response)
                        sys_prompts.append(sys_prompt)
                        user_prompts.append(user_prompt)
                if self.mode == "quality":
                    sampled_indices = np.random.choice(list(range(len(responses))), num_texts_per_agent, replace=False)
                    sampled_texts.extend(
                        [
                            "System Prompt: " + sys_prompts[i] + "\nUser Prompt: " + user_prompts[i] + "\nThe response of the agent you need to evaluate for: " + responses[i]
                            for i in sampled_indices
                        ]
                    )
                    
                else: # one long context per agent, because it is analyzing and summarizing one agent's trajectory.
                    sampled_indices = list(range(len(responses)))
                    sampled_texts.append("\n".join(
                        [
                            f"This is week {weeks[i]}. System Prompt: {sys_prompts[i]}\nUser Prompt: {user_prompts[i]}\nThe response of the agent you need to evaluate for: {responses[i]}" 
                            for i in sampled_indices
                        ]
                        )
                    )
        if self.mode == "quality":
            print(f"Sampled {len(sampled_texts)} texts for quality evaluation.")
            return sampled_texts
        else:
            return sampled_texts, sampled_agents_paths

    async def async_request_chat_completion(self, session, text, max_tokens=200, category="attitude"):
        """
        Asynchronously sends a request to Azure OpenAI's Chat Completions API.

        Args:
            session (aiohttp.ClientSession): The aiohttp session for making requests.
            text (str): The text to evaluate.
            deployment_id (str): The Azure OpenAI deployment ID.
            prompt_template (str): Optional prompt template for evaluation.
            max_tokens (int): The maximum number of tokens for the response.

        Returns:
            str: The evaluation result or an error message.
        """

        # Correct Chat Completions API endpoint URL
        
        headers = {
            "api-key": AZURE_API_KEY,
            "Content-Type": "application/json"
        }

        payload = {
            "messages": [
                {"role": "system", "content": self.system_prompt[category]},
                {"role": "user", "content": text}
            ],
            "max_tokens": max_tokens,
            "temperature": 0,
            "n": 1
        }

        max_retries = 10
        retry_delay = 1  # seconds
        for i in range(max_retries):
            try:
                async with session.post(self.url, headers=headers, json=payload) as response:
                    if response.status == 429:
                        retry_after = int(response.headers.get("Retry-After", 5))
                        print(f"Rate limit hit. Retrying in {retry_after} seconds...")
                        await asyncio.sleep(retry_after)
                        continue
                    elif response.status >= 400:
                        raise Exception(f"HTTP {response.status}: {await response.text()}")
                    result = await response.json()
                    result = result["choices"][0]["message"]["content"].strip()
                    if category == "analysis" or category == "meta_analysis":
                        return result
                    res_json = parse_json(result, variable_strings=["rating", "reasoning"])
                    if res_json is None:
                        print(f"Failed to parse JSON, retrying ({i+1}/{max_retries})...")
                        # short circuit to only parsing the rating
                        res_str = result.strip().replace('"', "").replace("'", "")
                        res, success = is_num(res_str)
                        if success:
                            return {"rating": res, "reasoning": "Could not parse reasoning."}
                        continue 
                    return res_json
            except Exception as e:
                await asyncio.sleep(retry_delay)
                continue
        return {"rating": 3, "reasoning": "Could not get a response from the model after retries."}
        

    async def evaluate_texts(self, texts, category="attitude", batch_size=1, max_tokens=200):
        """
        Evaluates a list of texts using Azure OpenAI Chat Completions API asynchronously in batches.

        Args:
            texts (list of str): A list of texts to evaluate.
            deployment_id (str): The Azure OpenAI deployment ID.
            category (str): The evaluation category.
            batch_size (int): Number of texts to process in a single batch.

        Returns:
            list of dict: List of evaluation results for each text.
        """
        async with aiohttp.ClientSession() as session:
            results = []
            for i in async_tqdm(range(0, len(texts), batch_size), desc=f"Evaluating texts of category={category}"):
                batch = texts[i:i + batch_size]
                tasks = [
                    self.async_request_chat_completion(session, text, category=category, max_tokens=max_tokens)  
                    for text in batch
                ]
                batch_results = await asyncio.gather(*tasks)
                results.extend(batch_results)  # Collect results from the current batch
            return results

async def main(args):
    # Example texts for evaluation
    
    np.random.seed(42)
    category_map = {
        "attitude": "attitude",
        "memory": "feed",
        "conversation": "prompt",
        "analysis": "attitude",
    }
   
    # Saving directory for LLM analysis
    save_dir = args.save_dir
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)
    
    judge = Judge(mode=args.mode)
    sampled_texts = []
    sampled_agent_paths = []

    if args.mode == "analysis":
        category = "analysis"
        for data_dir in args.data_dir:
            new_sampled_texts, sampled_agent_path = judge.sample_texts(data_dir, num_agents=args.num_agents, num_texts_per_agent=args.num_texts_per_agent, sample_stage=category_map[category])
            sampled_texts.extend(new_sampled_texts)
            sampled_agent_paths.extend(sampled_agent_path)
        assert len(sampled_agent_paths) == len(sampled_texts), print(f"sampled_agent_paths: {len(sampled_agent_paths)}, sampled_texts: {len(sampled_texts)}")
        evaluations = await judge.evaluate_texts(sampled_texts, category=category, max_tokens=500)
        assert len(sampled_agent_paths) == len(evaluations)
        with open(os.path.join(save_dir, f"evaluations_analysis.tsv"), "w") as f:
            f.write("agent_path\tanalysis\n")
            for i in range(len(sampled_texts)):
                f.write(f"{sampled_agent_paths[i]}\t{evaluations[i]}\n")
            f.close()
        # meta analysis
        meta_analysis_context = ""
        for i in range(len(evaluations)):
            meta_analysis_context += f"This is the analysis for agent {i}'s attitude changes: {evaluations[i]}.\n"
        meta_analysis = await judge.evaluate_texts([meta_analysis_context], category="meta_analysis",max_tokens=3000)
        with open(os.path.join(save_dir, f"meta_analysis.txt"), "w") as f:
            f.write(meta_analysis[0])
            f.close()
    elif args.mode == "meta_analysis":
        for data_dir in args.data_dir:
            analyses = []
            meta_analysis_context = ""
            with open(os.path.join(data_dir, "evaluations_analysis.tsv"), "r") as f:
                lines = f.readlines()
                for i, line in enumerate(lines):
                    if "\t" not in line:
                        continue
                    agent_path, analysis = line.split("\t")
                    analyses.append(analysis)
                    meta_analysis_context += f"This is the analysis for agent {i}'s attitude changes: {analysis}.\n"
            meta_analysis = await judge.evaluate_texts([meta_analysis_context], category="meta_analysis",max_tokens=3000)
            with open(os.path.join(save_dir, f"meta_analysis.txt"), "w") as f:
                f.write(meta_analysis[0])
                f.close()
    else:
        # Evaluate texts asynchronously
        for category in ["attitude", "memory", "conversation"][::-1]:
            sampled_texts = []
            for data_dir in args.data_dir:
                sampled_texts.extend(judge.sample_texts(data_dir, num_agents=args.num_agents, num_texts_per_agent=args.num_texts_per_agent, sample_stage=category_map[category]))
            # save the sampled_texts
            random_indices = np.random.choice(list(range(len(sampled_texts))), 20, replace=False)
            reduced_sampled_texts_for_human_eval = [sampled_texts[i] for i in random_indices]
            with open(os.path.join(save_dir, f"sampled_texts_{category}.txt"), "w") as f:
                for text in reduced_sampled_texts_for_human_eval:
                    f.write(text + "\n\n\n\n")
                f.close()
            evaluations = await judge.evaluate_texts(sampled_texts, category=category)
            scores = []
            with open(os.path.join(save_dir, f"evaluations_{category}.jsonl"), "w") as f:
                for e in evaluations:
                    f.write(json.dumps(e) + "\n")
                    scores.append(e["rating"])
                f.write(json.dumps({"mean": sum(scores)/len(scores), "std": np.std(scores)}))
                f.close()
            print(f"Evaluations saved to evaluations_{category}.jsonl")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("data_dir", type=str, help="Directory containing texts for evaluation", nargs="+")
    parser.add_argument("--mode", choices=["analysis", "quality", "meta_analysis"], type=str, default="quality")
    parser.add_argument("--num_agents", type=int, default=25, help="Number of agents to sample")
    parser.add_argument("--num_texts_per_agent", type=int, default=10, help="Number of texts to sample per agent")
    parser.add_argument("--save_dir", type=str, default="evals", help="Directory to save evaluation results")
    args = parser.parse_args()
    asyncio.run(main(args))